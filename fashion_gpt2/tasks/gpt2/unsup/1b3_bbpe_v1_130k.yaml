network:
  hidden_size: 2048
  n_embed: 2048  # vocab embedding
  n_inner: 8192
  n_head: 16
  n_layer: 24
  vocab_size: 130048
  max_position_embeddings: 2048
  layer_norm_epsilon: 1.0e-5
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0
  attn_pdrop: 0.1
  scale_attn_weights: true # TODO:
  scale_attn_by_inverse_layer_idx: false # TODO:
  reorder_and_upcast_attn: false # TODO:
  initializer_range: 0.013975424859373685
  gradient_checkpointing: false
  tie_weight: true
