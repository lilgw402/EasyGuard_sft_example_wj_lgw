log_every_n_steps: 50
precision: 'fp16'
strategy: 'deepspeed'
accelerator_kwargs:
  # only used for deepspeed strategy
  ds_config:
    steps_per_print: 50
    prescale_gradients: false
    zero_allow_untested_optimizer: true
    bf16:
      enabled: false
    fp16:
      enabled: true
    wall_clock_breakdown: false
    gradient_clipping: 1.0
    zero_optimization:
      stage: 3
      allgather_partitions: true
      reduce_scatter: true
      allgather_bucket_size: 50000000
      reduce_bucket_size: 50000000
      overlap_comm: true
      contiguous_gradients: true
      offload_optimizer:
        device: "none"
optimizer_kwargs:
  optimizer:
    type: "deepspeed.ops.adam.FusedAdam"
    params:
      lr: 1e-4
      betas:
      - 0.9
      - 0.95
      eps: 1.0e-08
      weight_decay: 0.1
      bias_correction: true
      adam_w_mode: true
  scheduler:
    type: "mariana.optim.lr_scheduler.get_cosine_schedule_with_warmup_lrdecay"
    total_steps_param_name: "num_training_steps"
    warmup_steps_param_name: "num_warmup_steps"
    interval: "step"
    params:
      warmup_step_rate: 0.005
      lr_end: 0.1
      lr_decay_rate: 0.8666666666666667
