log_every_n_steps: 10
logger: ['tracking', 'console']
precision: "fp16"
strategy: 'deepspeed'
accelerator_kwargs:
  # only used for deepspeed strategy
  ds_config:
    steps_per_print: 50
    prescale_gradients: false
    zero_allow_untested_optimizer: true
    bf16:
      enabled: false
    fp16:
      enabled: true
    wall_clock_breakdown: false
    zero_optimization:
      stage: 2
      allgather_partitions: true
      reduce_scatter: true
      allgather_bucket_size: 50000000
      reduce_bucket_size: 50000000
      overlap_comm: true
      contiguous_gradients: true
      offload_optimizer:
        device: "none"
optimizer_kwargs:
  optimizer:
    type: "deepspeed.ops.adam.FusedAdam"
    params:
      lr: 1e-5
      betas:
      - 0.9
      - 0.999
      eps: 1.0e-06
      weight_decay: 0.01
      bias_correction: true
      adam_w_mode: true
  scheduler:
    type: "fashBloom.optim.lr_scheduler.get_linear_schedule_with_warmup"
    total_steps_param_name: "num_training_steps"
    warmup_steps_param_name: "num_warmup_steps"
    interval: "step"
    params:
      warmup_step_rate: 0.005
      lr_end: 1e-6
