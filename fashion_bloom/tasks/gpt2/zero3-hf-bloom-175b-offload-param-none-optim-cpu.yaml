log_every_n_steps: 50
logger: ['tracking', 'console']
precision: 'fp16'
strategy: 'deepspeed'
accelerator_kwargs:
  zero3_lazy_init: true
  # only used for deepspeed strategy
  ds_config:
    steps_per_print: 50
    train_micro_batch_size_per_gpu: 1
    gradient_accumulation_steps: 1
    prescale_gradients: false
    zero_allow_untested_optimizer: true
    bf16:
      enabled: false
    fp16:
      enabled: true
    wall_clock_breakdown: false
    gradient_clipping: 1.0
    zero_optimization:
      stage: 3
      allgather_partitions: true
      reduce_scatter: true
      allgather_bucket_size: 5e7
      reduce_bucket_size: 5e7
      stage3_max_live_parameters: 1e9
      stage3_max_reuse_distance: 1e9
      overlap_comm: true
      contiguous_gradients: true
      offload_optimizer:
        # device: "none"
        pin_memory: true
        device: "cpu"
      offload_param:
        device: "none"
        # pin_memory: true
        # device: "cpu"
optimizer_kwargs:
  optimizer:
    type: "deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam"
    # type: "torch.optim.AdamW"
    # type: "deepspeed.ops.adam.FusedAdam"
    params:
      lr: 1.0e-05
      betas:
      - 0.9
      - 0.999
      eps: 1.0e-06
      weight_decay: 0.01
      bias_correction: true
      adam_w_mode: true
  scheduler:
    type: "fashBloom.optim.lr_scheduler.get_linear_schedule_with_warmup"
    total_steps_param_name: "num_training_steps"
    warmup_steps_param_name: "num_warmup_steps"
    interval: "step"
    params:
      warmup_step_rate: 0.005
      lr_end: 1e-6
